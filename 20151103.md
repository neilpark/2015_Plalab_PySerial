# 웹 정보 가져오기
## 필요 라이브러리
### mechanize
* Browser 다룰 수 있도록 해주는 라이브러리
* python 2.X 지원 : 3.X 미지원
* [참고 URL](http://wwwsearch.sourceforge.net/mechanize/)

``` python
from mechanize import Browser, urlopen

def init_browser():
    # Browser 초기화
    browser = Browser()

    browser.set_handle_robots(False)
    browser.set_handle_equiv(False)
    browser.set_handle_redirect(True)
    browser.set_handle_referer(True)
    #browser.set_debug_http(True)
    
    browser.addheaders = [('User-agent', 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125')]
    browser.addheaders = [('Connection', 'keep-alive')]
    browser.addheaders = [('Accept', 'text/html')]
    browser.addheaders = [('Accept-Encoding', 'gzip,deflate,sdch')]
    browser.addheaders = [('Accept-Language', 'en-US,en;q=0.8,ko;q=0.6')]
```

### beautifulsoup
* html을 파싱하여 원하는 단어를 기준으로 닫힌 태그 영역까지를 return 해준다.
* [참고 URL](http://coreapython.hosting.paran.com/etc/beautifulsoup4.html)
 
```python
from BeautifulSoup import BeautifulSoup

def connect_and_get_html (browser, url):
    # Browser를 통해 해당 url에 접속하여 html을 가져온다.
    response = browser.open(url)
    html = response.read()
    response.close()
    return html


def parsingHTML(browser, url):
    html = connect_and_get_html(browser, url)
    # 가지고 온 html을 BeutifulSoup 라이브러리로 전달한다.
    parsed_html = BeautifulSoup(html)
    # 이후 필요한 정보들이 있는 태그를 찾는다.
    target_frame = parsed_html.findAll('frame')
    trs = parsed_html.findAll('tr')
    tds = parsed_html.findAll('td')
```
 
